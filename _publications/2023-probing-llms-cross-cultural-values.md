---
title: "Probing Pre-Trained Language Models for Cross-Cultural Differences in Values"
collection: publications
permalink: /publication/2023-probing-llms-cross-cultural-values
excerpt: 'This paper delves into the exploration of social, cultural, and political values encoded in Pre-Trained Language Models (PTLMs) and investigates how these values vary across cultures. Introducing probes for systematic study, the research reveals that PTLMs capture cultural differences in values, although alignment with established cross-cultural value surveys is weak.'
date: 2023-05-03
venue: 'Wiki Workshop at the Web Conference'
paperurl: 'https://arxiv.org/abs/2203.13722'
citation: 'Arora, A., Kaffee, L. A., & Augenstein, I. (2023). Probing Pre-Trained Language Models for Cross-Cultural Differences in Values. In Proceedings of the First Workshop on Cross-Cultural Considerations in NLP (C3NLP) at EACL 2023.'
---
Language embeds information about social, cultural, and political values people hold. Prior work has explored social and potentially harmful biases encoded in Pre-Trained Language models (PTLMs). However, there has been no systematic study investigating how values embedded in these models vary across cultures. In this paper, we introduce probes to study which values across cultures are embedded in these models, and whether they align with existing theories and cross-cultural value surveys. We find that PTLMs capture differences in values across cultures, but those only weakly align with established value surveys. We discuss implications of using mis-aligned models in cross-cultural settings, as well as ways of aligning PTLMs with value surveys.

[Full paper here](https://arxiv.org/abs/2203.13722)